\section{Related work}

Over the last 40 years a large variety of algorithms have been invented 
to cluster graphs. Most of these methods were based on different 
measures of an optimal clustering all of which were usually np-hard to 
derive optimal solutions for. This led to various algorithms attacking 
relaxed versions of these problems with different amount of success.  
The field has seen a boost in recent years due to the increased demands 
for handling large data that are usually neatly abstracted as graphs 
\cite[p.  2]{fortunato2010}.  Examples include social networks and the 
world wide web.

In this paper I don't intend to do a thorough review of the field, 
something which many better abled people have already done before 
me\footnote{Examples include \cite{newman2004}, \cite{schaeffer2007} and 
\cite{fortunato2010}}. Instead I have tried to pick out a few distinct 
algorithms that are good enough that each could have been a candidate 
for a clustering algorithm used for grouping the scientific articles.


% von2007 : schaeffer
% fortunato2010 : community detection in graphs
% lancichinetti2008 : comparison
% newman2005 : short and old overview

\subsection{How to measure clustering}

The central difficulty when proposing a new clustering algorithm is to 
measure how it compares with other algorithms. Since there is no agreed 
upon definition on what makes a good cluster, there is naturally no 
definitive candidate for measuring the ``goodness'' of a cluster.  
Historically there has been several different approaches.  
\cite{zachary1977} did a study on the relationships between members of a 
karate club which eventually split into two factions which has since 
then been a benchmark for clustering algorithms. If an algorithm on the 
basis of the social graph can accurately predict the two factions in the 
split, then it has been judged as useful\footnote{See for example 
\cite{girvan2002} and \cite{lancichinetti2009}}.

Fortunately better metrics have been proposed since. One of the most 
used has been the measure of \emph{modularity} proposed by 
\cite{girvan2002} and defined as:
\begin{eqnarray}
	Q & = & (\mbox{fraction of edges within communities}) \\
	  & & - (\mbox{expected fraction of such edges}) \\
	& = & \frac{1}{2m} \sum_{i,j} \left[ A_{ij} - \frac{k_i k_j}{2m} \
\right] \delta(c_i, c_j)
\end{eqnarray}
Where $m=\frac{1}{2} \sum_{ij} A_{ij}$, $k_i = \sum_j A_{ij}$ and 
$A_{ij}$ is the weight between the edge connecting $i$ and $j$.  
$\delta(c_i, c_j)$ is the Kroneker Delta which is 1 only if the 
community assignment $c_i$ is equal to the community assignment $c_j$.  
The community measure has been the de-facto standard but it is not 
without shortcomings. It has been shown by \cite{brandes2007} that the 
metric doesn't work well with small communities.

An alternative measure called Surprise was introduced by 
\cite{arnau2005} and has later been shown to perform better than the 
community measure in a series of cases by \cite{aldecoa2011}, although 
it has yet to be independently adopted verified and adopted by other 
researchers. The idea behind Surprise is to model the distributions of 
intra- and inter-community links with a cumulative hypergeometric 
distribution. It is defined as follows:
\begin{equation}
	S = -log \sum_{j=p}^{Min(M,n)} \frac{\binom{M}{j} \
	\binom{F-M}{m-j}}{\binom{F}{n}}
\end{equation}
Here $F$ is the maximum possible number of edges in the graph, that is 
$F = (n^2-n)/2$, while $M$ is the maximum number of edges within a 
cluster $A$ of size $k$, that is $M = (k^2-k)/2$. $p$ is the actual 
number of links found in the community. The authors have not proposed a 
similar formula for weighted graphs to the author's knowledge.

An alternative method of testing graph clustering algorithms is to 
design a series of graphs with known partitioning and see how well the 
algorithm in question predicts these graphs. The most widely used of 
such graphs was introduced by \cite{girvan2002}. They consisted of 128 
vertices with four clusters of even size. The average degree of each 
vertex was 16. A parameter $k_{out}$ would designate how many edges a 
vertex would have leading to other clusters. With $k_{out} < 8$ a good 
clustering algorithm should be expected to find the original 
partitioning.  However these graphs are a very limited subset of the 
real graphs that clustering algorithms are supposed to work with. In 
particular they are small, the clusters are of an even size and the 
vertices have very similar degrees\footnote{As pointed out by 
\cite{lancichinetti2008}}.  These shortcomings lead to misleading 
benchmark results for algorithms that are not suited to work on for 
example graphs with clusters of very different sizes.

To improve on these shortcomings, another group of random graphs was 
introduced in 2009 by \cite{lancichinetti2008}. With the implicit 
assumption that degree and community size in real graphs are guided by 
power laws, they construct a set of graphs usually denominated as 
\emph{LFR} graphs where each vertex is given a degree taken from a power 
law distribution with exponent $\gamma$ such that the average degree is 
$k$.  Each vertex keeps a fraction $1-\mu$ of its edges within its own 
cluster and the remaining fraction $\mu$ with vertices outside its 
cluster.  The sizes of clusters are also taken from a power law 
distribution. The random graphs are then constructed by assigning the 
vertices randomly to clusters as long as each vertex has less 
inter-cluster links than the size of the cluster. The graph is then 
adjusted to make sure the amount of inter-cluster versus intra-cluster 
links is given according to $\mu$.

The \emph{LFR} graphs are like their predecessors the \emph{GN} graphs a 
limited benchmark in the sense that they will reward algorithms that are 
particularly good at clustering graphs that correspond to the particular 
structure determined by the power distribution of of edges and cluster 
sizes. These graphs has later been used by \cite{lancichinetti2009} in a 
benchmark of 12 clustering algorithms, many of which are mentioned in 
the following section of this paper. The \emph{LFR} are also used by 
\cite{aldecoa2010} to show that \emph{Surprise} is better correlated 
with the true clustering than \emph{Modularity} in a series of tests 
with various algorithms.

\subsection{Girvan Newman clustering}
If we look at the problem of clustering a graph as a problem of removing 
edges until we arrive at a set amount of communities, we need a good 
measure for what edges should be removed. The Girvan-Newman algorithm 
(in short \emph{GN}) proposes that we remove edges based on a measure of 
``betweenness'', that is how much a particular edge $e_{ij}$ is situated 
between clusters. The betweenness of $e_{ij}$ is calculated by counting 
how many shortest paths between pairs of vertices that run through 
$e_{ij}$. In case there is ($k > 1$) shortest paths, each path 
traversing $e_{ij}$ contribues $\frac{1}{k}$ to the count.  If we define 
$\sigma_{st}$ as the number of shortest paths between vertex $s$ and 
$t$, and $\sigma_{st}(e_{ij})$ as the number of shortest paths between 
$s$ and $t$ traversing $e_{ij}$ we can write betweenness as
\begin{equation}
	C_B(e_{ij})= \sum_{s \neq v \neq t \in V} \
	\frac{\sigma_{st}(e_{ij})}{\sigma_{st}}
\end{equation}
Using this measure we can iteratively construct a dendrogram by 
calculating all the shortest paths of the graph, then finding the edge 
with the highest betweenness and finally remove this edge and repeat. A 
common way to find the best amount of clusters is to stop removing edges 
when the highest value measure of \emph{Modularity} is obtained.
The complexity of this algorithm is $O(n^3)$ in the general case, but it
can be calculated slightly faster when the graph is sparse. The 
algorithm does not perform particularly well on random 
graphs\footnote{According to \cite{lancichinetti2009}}, but it is very 
simple to implement and requires no additional parameters.


% https://en.wikipedia.org/wiki/Girvan%E2%80%93Newman_algorithm
% girvan2002

\subsection{Spectral clustering}
From spectral graph theory we get the spectral clustering algorithm 
which in reality is an ensemble of different algorithms all solving the 
problem of clustering a similarity graph in similar ways. The general 
idea is to use the eigenspectrum of the graph Laplacian to derive a set 
of $k$ othonormal vectors that are trivial to cluster using a k-means 
algorithm \cite[pp. 3]{von2007}. The parameter $k$ needs to be decided 
in advance. The differences across spectral clustering algorithms 
usually lie in how the graph Laplacian is defined.  One approach is to 
define the Laplacian $L$ for a graph $G$ as $L = D - A$.  However it has 
been shown that using the normalized Laplacian yields better results.  
One way to define the normalized Laplacian as used by \cite{ng2002} is 
$L = I - D^{-1/2} A_G D^{-1/2}$.

Spectral clustering suffers from the fact that we need to decide the 
amount of clusters to output before running the algorithm. This can be 
avoided by calculating clusterings for $2$ up to $k$ groupings and then 
find the number that optimizes a given measure of quality, for example 
the measure of \emph{Modularity}. The complexity of calculating the 
eigenvectors is $O(n^3)$ in the worst case. This can be sped up using 
the Lanczos method\footnote{See for example \cite{golub1996}}, but the 
worst case complexity is still the same. As for the quality of a 
solution derived with spectral clustering a lot of research has been 
made on the subject, but there has not been any definite results. In 
\cite{lancichinetti2009} a spectral method was compared to several other 
algorithms and not found particularly well performing.  A big advantage 
of spectral clustering is that it is simple to implement.
% ng2002 the algorithm I use
% von2007 tutorial

\subsection{An approach from Information Theory}
Instead of looking at a graph theoretic measures of clusterness, an 
alternative idea proposed by \cite{rosvall2008} is to turn to 
Information Theory. The approach they propose is to look at clustering 
as a problem of how best to encode the path of a random walk. A random 
walk in a weighted graph is a traversal from vertex to vertex where a 
step from a vertex $v_i$ to another vertex $v_j$ connected to $v_i$ by 
the edge $e_{ij}$ happens with a probability $p_{ij}$. We can calculate 
$p_{ij}$ as $\frac{w_{ij}}{d_i}$. This means that edges with a high 
weight are traveled often and vertices with a high degree are visited 
often. If we choose to characterize the random walk as a walk that most 
of the time remains within clusters and occasionally goes between 
clusters we can encode this walk by attributing each cluster a codeword, 
and then in each cluster reuse the same group of codewords. This way we 
can derive the average description length of a single step as follows:
\begin{equation}
	L(\textbf{M}) = q H(\mathcal{Q}) + \sum_{i=1}^{m} p^i \
	H(\mathcal{P}^i)
\end{equation}
In this equation $H(\mathcal{Q})$ is the entropy of codewords assigned 
to the clusters and $H(\mathcal{P}^i)$ is the entropy of the 
within-cluster movements seen as a string of codewords describing the 
path. This includes an extra word to designate that we are leaving the 
cluster. $q$ is the probability that we switch between clusters at any 
given step, while $p$ is the fraction of within-cluster movements that 
occur in cluster $i$ plus the probability of exiting cluster $i$.  
Optimizing this measure leads us to an optimal clustering.  In practice 
this is done by exploring the space of possible clusters using a breadth 
first search and refining these results using simulated annealing.

This approach has several positive properties. It doesn't need 
parameters and is relatively fast with a complexity of $O(m)$ and in 
\cite{lancichinetti2009} it is rated as one of the best performing 
algorithms. However the implementation is relatively involved using 
several different concepts stitched together. This algorithm is labeled 
\emph{Infomod} when referenced later.
% rosvall2008

\subsection{The Louvain Method}
The \emph{Louvain} Method was introduced in 2008 by \cite{blondel2008} 
and made it possible to feasibly cluster more than 100 million vertices 
as compared to the few million vertices which had formerly been possible 
with the fastest methods. The method is based on the maximizing the 
\emph{Modularity} of a graph iteratively. This is done efficiently 
because the gain in modularity from moving a vertex $v_i$ to a community 
$C$ can be calculated easily:
\begin{multline}
	\Delta Q = \left[ \frac{\sum_{in} + 2 k_{i,in}}{2m} - \
	\left(\frac{\sum_{tot} + k_i}{2m} \right)^2 \right] \\
	- \left[\frac{\sum_{in}}{2m} - \left(\frac{\sum_{tot}}{2m} \
	\right)^2 - \left( \frac{k_i}{2m} \right)^2 \right]
\end{multline}
Here as in the definition of \emph{Modularity}, $m=\frac{1}{2} \sum_{ij} 
A_{ij}$, $k_i = \sum_j A_{ij}$ and $k_{i,in}$ is the sum of weights of 
the edges from $v_i$ to vertices in $C$.  $\sum_{in}$ is the sum of the 
weights of the edges of $C$ while $\sum_{tot}$ is the sum of all edges 
with at least one vertex in $C$.  The algorithm works by assigning each 
vertex to its proper cluster and then for each iteration reassign 
vertices to neighboring communities if it increases the 
\emph{Modularity}.  These iterations are carried out until there are no 
reassignments in which case an initial clustering has been reached.  For 
graphs consisting of a large number of vertices, the algorithm can now 
be repeated, treating each of the resulting clusters as a vertex and 
repeating the same procedure.

In terms of speed and simplicity the \emph{Louvain} method is one of the 
best methods available today. It has a computational complexity of 
$O(m)$ and is very simple to implement. What's more, there is no need to 
specify how many clusters we would expect and the individual cluster 
sizes have shown to be sensible in real world trials.  As for the 
quality of the solution it compares favorably to other clustering 
methods in the comparison on \emph{LFR} graphs made by 
\cite{lancichinetti2009}.
% blondel2008


% \subsection{Expectation minimization clustering}
% leicht2008
% not suitable since it works in directed graphs
\subsection{Potts Model Approach}
Inspired by the \emph{Louvain} Method an algorithm based on Potts model 
was introduced by \cite{ronhovde2009}. Instead of working with the 
\emph{Modularity} it is based on Potts Model Hamiltonian which is 
defined as follows:
\begin{equation}
	\mathcal{H}(\{c\}) = \frac{1}{2} \sum_{i \neq j} (A_{ij} - \
	\gamma \mathbf{1}_{\{A_{ij} = 0\}} \delta(c_i, c_j)
\end{equation}
$\delta(c_i, c_j)$ is the Kroneker Delta and is 1 only if the community 
assignment of $v_i$, $c_i$ is equal to the community assignment of 
$v_j$, $c_j$. $\gamma$ is a parameter specified before the algorithm is 
run. As with the \emph{Louvain} method each vertex is assigned to its 
proper cluster and then iteratively reassigned to neighboring clusters 
based on the above measure. Different from the \emph{Louvain} method, 
this assignment is done several times and the best cluster is picked 
out.  Given the partitions $A$ and $B$ this is done with measures on the 
entropy ($H(A)$ and $H(B)$), the mutual information ($I(A,B)$), the 
normalized mutual information ($I_N(A,B)$) and the variation of 
information ($V(A,B)$).  For the graph they are defined as follows:
\begin{eqnarray}
	H(A) & = & - \sum_{k=1}^{q_A} \frac{n_k}{n} log \frac{n_k}{N} \\
	I(A,B) & = & \sum_{i=1}^{q_A} \sum_{j=1}^{q_B} \frac{n_{ij}}{N} \
	log \frac{n_{ij}N}{v_i v_j} \\
	I_N(A,B) & = & \frac{2I(A,B)}{H(A) + H(B)} \\
	V(A,B) & = & H(A) + H(B) - 2I(A,B)
\end{eqnarray}
Here $q_A$ is the amount of clusters in $A$ where as $n$ is the total 
amount of vertices and $n_k$ the total amount of vertices in cluster $k$ 
of partition $A$. $n_{ij}$ is the amount of vertices shared by cluster 
$i$ of partition $A$ and cluster $j$ of partition $B$. For each run, the 
gamma parameter is adjusted and another clustering is calculated. Then 
the best clustering is picked by looking at the regions of the graph 
that are strongly correlated measured by high $I_N$ and low $V$.
The algorithm performs well according to \cite{lancichinetti2009} and is 
very fast with a complexity of $O(m^\beta log n)$, $\beta \approx 1.3$.  
It is however complex to implement and requires that several model 
parameters are supplied. In the following this algorithm is referred to 
as \emph{Potts}.

\subsection{An overview}

\begin{table}
	\small
\begin{tabular}{l*{4}{c}}
	Author(s) & Year & Label & Order \\
	\hline
	\noalign{\smallskip} 
	Girvan and Newman & 2002 & \emph{GN} & $O(n^3)$ \\
	Ng \& al. & 2002 & \emph{Spectral} & $O(n^3)$ \\
	Rosvall and Bergstrom & 2008 & \emph{Infomap} & $O(m)$ \\
	Blondel \& al. & 2008 & \emph{Louvain} & $O(m)$ \\
	Ronhovede and Nussinov & 2009 & \emph{Potts} & $O(m^\beta log n)$ \\
\end{tabular}
\caption{Complexity Overview}
\label{table:complexity}
\end{table}

As seen in table \ref{table:complexity}, the differences between the 
fastest and slowest algorithms are vast. With more than a few thousand 
vertices, calculating the spectral clustering or using the Girvan-Newman 
method will be unfeasible on most hardware.  The quality of each 
algorithm as measured by benchmarking them on the \emph{LFR} graphs show 
that the three best algorithms, \emph{Louvain}, \emph{Potts} and 
\emph{Infomap} perform well on this type of random graphs, with 
\emph{Infomap} performing better on larger graphs\footnote{A much more 
detailed analysis of these results can be found in 
\cite{lancichinetti2009}}. 
